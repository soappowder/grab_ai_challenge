{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- place all test data (in csv format) in `/grab_ai_challenge/data/safety/test` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import joblib\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config project folder\n",
    "os.chdir('../')\n",
    "\n",
    "# config notebook display\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all test data in the data/safety/ folder at once\n",
    "\n",
    "def load_data(kind):\n",
    "    path ='data/safety/'+ kind\n",
    "    all_files = glob.glob(os.path.join(path, \"*.csv\")) \n",
    "    features = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = load_data(kind=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* some records have abnormally big value for \"second\" (data error) --> remove these records from features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove records with abnormally big value for \"second\" (> e+05): \n",
    "abnormality_threshold = 10**5\n",
    "features_df = features_df.loc[features_df.second<=abnormality_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* since training data only include trips <= 1 hr, split test trips longer than 1 hr into multiple **sub-trips**, label them with sub ID\n",
    "    * second in [0, 3600) : subID = 1\n",
    "    * second in [3600, 7200) : subID = 2\n",
    "    * second in [7200, 10800) : subID = 3\n",
    "* bookingID_new = bookingID + subID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate subID from second\n",
    "features_df['subID'] = np.floor(features_df.second/60/60).astype(int)+1\n",
    "\n",
    "# transform second by subID\n",
    "features_df['second'] = features_df.second - (features_df.subID-1)*60*60\n",
    "\n",
    "# combine bookingID with subID\n",
    "features_df['bookingID_new'] = features_df.bookingID.astype(str) + \"-\" + features_df.subID.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sort bookings by bookingID, subID and second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.sort_values(['bookingID','subID','second'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature scaling\n",
    "- the features have many outliers and these outliers are important (indicative of dangerous driving behaviour), hence, we do not want to clip the data\n",
    "- StandardScaler and MinMaxScaler are sensitive to the presence of outliers, hence, we'll use RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### acceleration\n",
    "* shifting acceleration_y by gravitational force g = 9.81\n",
    "* since acceleration_x is in the range of [0.5, -0.5], we don't want to scale acceleration again\n",
    "* since 0 acceleration has special meaning, we don't want to center acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 9.81 # gravitational force\n",
    "features_df['acceleration_y_shifted'] = features_df['acceleration_y'] - g\n",
    "features_df.drop('acceleration_y', axis=1, inplace=True)\n",
    "features_df.rename(columns={'acceleration_y_shifted':'acceleration_y'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPS\n",
    "- `scaler_gps = RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True, with_scaling=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use scaler fitted on training features to transform test features\n",
    "\n",
    "scaler_gps = joblib.load('model/scaler_gps.save') \n",
    "features_df[['Accuracy', 'Bearing','Speed']] = scaler_gps.transform(features_df[['Accuracy', 'Bearing','Speed']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gyro\n",
    "- `scaler_gyro = RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=False, with_scaling=True)`\n",
    "- since 0 gyro has special meaning, we don't want to center gyro (it's already quite centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use scaler fitted on training features to transform test features\n",
    "scaler_gyro = joblib.load('model/scaler_gyro.save') \n",
    "features_df[['gyro_x', 'gyro_y','gyro_z']] = scaler_gyro.transform(features_df[['gyro_x', 'gyro_y','gyro_z']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to prepare for CNN, pad all sub-trips with 0 so that they have the same length (1 hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table keys\n",
    "unique_bookingID = features_df[['bookingID_new']].drop_duplicates().sort_values('bookingID_new')\n",
    "unique_second = pd.DataFrame(data=list(np.arange(0,3600,1.0)), columns=[\"second\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cartesian product with keys\n",
    "unique_bookingID['key'] = 0\n",
    "unique_second['key'] = 0\n",
    "bookingID_second = unique_bookingID.merge(unique_second, on='key', how='outer').drop(columns=['key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer join with features by bookingID and second\n",
    "features_df = bookingID_second.merge(features_df, on=['bookingID_new', 'second'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding with 0\n",
    "features_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check features df shape after padding\n",
    "features_df.shape[0]==unique_bookingID.shape[0]*unique_second.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert features df to 3d array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* array dimension = (#bookings, #seconds, #features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPS features\n",
    "\n",
    "print(time.strftime(\"%H:%M:%S\"),\": start Accuracy\")\n",
    "accuracy_arr = features_df.pivot_table(index=\"bookingID_new\", columns=\"second\", values=\"Accuracy\").values\n",
    "\n",
    "print(time.strftime(\"%H:%M:%S\"),\": start Bearing\")\n",
    "bearing_arr = features_df.pivot_table(index=\"bookingID_new\", columns=\"second\", values=\"Bearing\").values\n",
    "\n",
    "print(time.strftime(\"%H:%M:%S\"),\": start Speed\")\n",
    "speed_arr = features_df.pivot_table(index=\"bookingID_new\", columns=\"second\", values=\"Speed\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acceleration\n",
    "\n",
    "print(time.strftime(\"%H:%M:%S\"),\": start acceleration_x\")\n",
    "acceleration_x_arr = features_df.pivot_table(index=\"bookingID_new\", columns=\"second\", values=\"acceleration_x\").values\n",
    "\n",
    "print(time.strftime(\"%H:%M:%S\"),\": start acceleration_y\")\n",
    "acceleration_y_arr = features_df.pivot_table(index=\"bookingID_new\", columns=\"second\", values=\"acceleration_y\").values\n",
    "\n",
    "print(time.strftime(\"%H:%M:%S\"),\": start acceleration_z\")\n",
    "acceleration_z_arr = features_df.pivot_table(index=\"bookingID_new\", columns=\"second\", values=\"acceleration_z\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gyro\n",
    "\n",
    "print(time.strftime(\"%H:%M:%S\"),\": start gyro_x\")\n",
    "gyro_x_arr = features_df.pivot_table(index=\"bookingID_new\", columns=\"second\", values=\"gyro_x\").values\n",
    "\n",
    "print(time.strftime(\"%H:%M:%S\"),\": start gyro_y\")\n",
    "gyro_y_arr = features_df.pivot_table(index=\"bookingID_new\", columns=\"second\", values=\"gyro_y\").values\n",
    "\n",
    "print(time.strftime(\"%H:%M:%S\"),\": start gyro_z\")\n",
    "gyro_z_arr = features_df.pivot_table(index=\"bookingID_new\", columns=\"second\", values=\"gyro_z\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack all features together to create a 3d array\n",
    "test_arr = np.stack((accuracy_arr, bearing_arr, speed_arr, \n",
    "                     acceleration_x_arr, acceleration_y_arr, acceleration_z_arr, \n",
    "                     gyro_x_arr, gyro_y_arr, gyro_z_arr), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save processed test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test array\n",
    "save_path = 'data/safety/test_arr.npy'\n",
    "np.save(save_path, test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test bookingID and subID\n",
    "unique_bookingID.drop(columns=['key'], inplace=True)\n",
    "unique_bookingID[['bookingID', 'subID']] = pd.read_csv(\n",
    "    StringIO(features_df['bookingID_new'].to_csv(None,index=None, header=False)),sep='-',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_bookingID.to_csv('data/safety/test_id.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
